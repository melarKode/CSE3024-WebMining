{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "Python 3.8.3 64-bit",
   "display_name": "Python 3.8.3 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "f76182b4c67eda9e9be9e2b182aeb8560210f37ed8d48fe08cf5af4d01ee2941"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os \n",
    "import re\n",
    "import operator\n",
    "import pickle\n",
    "import nltk \n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from collections import defaultdict\n",
    "from nltk.corpus import wordnet as wn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "news = pd.read_json('https://raw.githubusercontent.com/zayedrais/DocumentSearchEngine/master/data/newsgroups.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,txt in enumerate(news['content']):\n",
    "    #print(i)\n",
    "    subject = re.findall('Subject:(.*\\n)',txt)\n",
    "    if (len(subject) !=0):\n",
    "        news.loc[i,'Subject'] =str(i)+' '+subject[0]\n",
    "    else:\n",
    "        news.loc[i,'Subject'] ='NA'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_news =news[['Subject','content']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_news['content']=[entry.lower() for entry in df_news['content']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "SW=['subject:','organization:','thanks','thank','re:']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sw in SW:\n",
    "    df_news.content=df_news.content.replace(to_replace=sw,value='',regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_news.content =df_news.content.replace(to_replace='from:(.*\\n)',value='',regex=True) #remove from to email \n",
    "df_news.content =df_news.content.replace(to_replace='lines:(.*\\n)',value='',regex=True)\n",
    "df_news.content =df_news.content.replace(to_replace='[!\"#$%&\\'()*+,/:;<=>?@[\\\\]^_`{|}~]',value=' ',regex=True) #remove punctuation except\n",
    "df_news.content =df_news.content.replace(to_replace='-',value=' ',regex=True)\n",
    "df_news.content =df_news.content.replace(to_replace='\\s+',value=' ',regex=True)    #remove new line\n",
    "df_news.content =df_news.content.replace(to_replace='  ',value='',regex=True)                #remove double white space\n",
    "df_news.content =df_news.content.apply(lambda x:x.strip())  # Ltrim and Rtrim of whitespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_news.Subject =df_news.Subject.replace(to_replace='Re:',value='',regex=True)\n",
    "df_news.Subject =df_news.Subject.replace(to_replace='[!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~]',value=' ',regex=True)\n",
    "df_news.Subject =df_news.Subject.replace(to_replace='\\s+',value=' ',regex=True)    #remove new line\n",
    "df_news.Subject =df_news.Subject.replace(to_replace='  ',value='',regex=True)    #remove double white space\n",
    "df_news.Subject =df_news.Subject.apply(lambda x:x.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,sen in enumerate(df_news.content):\n",
    "    if len(sen.strip()) ==0:\n",
    "        print(str(i))\n",
    "        #file_data.text[i] = np.nan\n",
    "        df_news=df_news.drop(str(i),axis=0).reset_index().drop('index',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_news['Word tokenize']= [word_tokenize(entry) for entry in df_news.content]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wordLemmatizer(data):\n",
    "    tag_map = defaultdict(lambda : wn.NOUN)\n",
    "    tag_map['J'] = wn.ADJ\n",
    "    tag_map['V'] = wn.VERB\n",
    "    tag_map['R'] = wn.ADV\n",
    "    file_clean_k =pd.DataFrame()\n",
    "    for index,entry in enumerate(data):\n",
    "        \n",
    "        # Declaring Empty List to store the words that follow the rules for this step\n",
    "        Final_words = []\n",
    "        # Initializing WordNetLemmatizer()\n",
    "        word_Lemmatized = WordNetLemmatizer()\n",
    "        # pos_tag function below will provide the 'tag' i.e if the word is Noun(N) or Verb(V) or something else.\n",
    "        for word, tag in pos_tag(entry):\n",
    "            # Below condition is to check for Stop words and consider only alphabets\n",
    "            if len(word)>1 and word not in stopwords.words('english') and word.isalpha():\n",
    "                word_Final = word_Lemmatized.lemmatize(word,tag_map[tag[0]])\n",
    "                Final_words.append(word_Final)\n",
    "            # The final processed set of words for each iteration will be stored in 'text_final'\n",
    "                file_clean_k.loc[index,'Keyword_final'] = str(Final_words)\n",
    "                file_clean_k.loc[index,'Keyword_final'] = str(Final_words)\n",
    "                #file_clean_k=file_clean_k.replace(to_replace =\"\\[.\", value = '', regex = True)\n",
    "                #file_clean_k=file_clean_k.replace(to_replace =\"'\", value = '', regex = True)\n",
    "                #file_clean_k=file_clean_k.replace(to_replace =\" \", value = '', regex = True)\n",
    "                #file_clean_k=file_clean_k.replace(to_replace ='\\]', value = '', regex = True)\n",
    "    return file_clean_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(11314, 3)"
      ]
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "source": [
    "df_news.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                       Keyword_final\n",
       "0  ['car', 'nntp', 'post', 'host', 'university', ...\n",
       "1  ['si', 'clock', 'poll', 'final', 'call', 'summ...\n",
       "2  ['pb', 'question', 'purdue', 'university', 'en...\n",
       "3  ['weitek', 'harris', 'computer', 'system', 'di...\n",
       "4  ['shuttle', 'launch', 'question', 'smithsonian...\n",
       "5  ['reword', 'second', 'amendment', 'idea', 'vtt...\n",
       "6  ['brain', 'tumor', 'treatment', 'reply', 'bmde...\n",
       "7  ['ide', 'vs', 'scsi', 'new', 'mexico', 'state'...\n",
       "8  ['win', 'icon', 'help', 'please', 'university'...\n",
       "9  ['sigma', 'design', 'double', 'article', 'univ..."
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Keyword_final</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>['car', 'nntp', 'post', 'host', 'university', ...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>['si', 'clock', 'poll', 'final', 'call', 'summ...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>['pb', 'question', 'purdue', 'university', 'en...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>['weitek', 'harris', 'computer', 'system', 'di...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>['shuttle', 'launch', 'question', 'smithsonian...</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>['reword', 'second', 'amendment', 'idea', 'vtt...</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>['brain', 'tumor', 'treatment', 'reply', 'bmde...</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>['ide', 'vs', 'scsi', 'new', 'mexico', 'state'...</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>['win', 'icon', 'help', 'please', 'university'...</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>['sigma', 'design', 'double', 'article', 'univ...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "source": [
    "df_clean = wordLemmatizer(df_news['Word tokenize'][0:10]) \n",
    "df_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean=df_clean.replace(to_replace =\"\\[.\", value = '', regex = True)\n",
    "df_clean=df_clean.replace(to_replace =\"'\", value = '', regex = True)\n",
    "df_clean=df_clean.replace(to_replace =\" \", value = '', regex = True)\n",
    "df_clean=df_clean.replace(to_replace ='\\]', value = '', regex = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df =pd.read_json('https://raw.githubusercontent.com/zayedrais/DocumentSearchEngine/master/data/WordLemmatize20NewsGroup.json')\n",
    "df_news['Clean_Keyword'] =df['Clean_Keyword']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                          Subject  \\\n",
       "0                              0 WHAT car is this   \n",
       "1                      1 SI Clock Poll Final Call   \n",
       "2                                  2 PB questions   \n",
       "3                                  3 Weitek P9000   \n",
       "4                       4 Shuttle Launch Question   \n",
       "...                                           ...   \n",
       "11309                   11309 Migraines and scans   \n",
       "11310             11310 Screen Death Mac Plus 512   \n",
       "11311  11311 Mounting CPU Cooler in vertical case   \n",
       "11312                  11312 Sphere from 4 points   \n",
       "11313                       11313 stolen CBR900RR   \n",
       "\n",
       "                                                 content  \n",
       "0      what car is this nntp posting host rac3.wam.um...  \n",
       "1      si clock poll final call summary final call fo...  \n",
       "2      pb questions... purdue university engineering ...  \n",
       "3      weitek p9000 harris computer systems division ...  \n",
       "4      shuttle launch question smithsonian astrophysi...  \n",
       "...                                                  ...  \n",
       "11309  migraines and scans distribution world inventi...  \n",
       "11310  screen death mac plus 512 tufts university med...  \n",
       "11311  mounting cpu cooler in vertical case mail grou...  \n",
       "11312  sphere from 4 points central research lab. hit...  \n",
       "11313  stolen cbr900rr california institute of techno...  \n",
       "\n",
       "[11314 rows x 2 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Subject</th>\n      <th>content</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0 WHAT car is this</td>\n      <td>what car is this nntp posting host rac3.wam.um...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1 SI Clock Poll Final Call</td>\n      <td>si clock poll final call summary final call fo...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2 PB questions</td>\n      <td>pb questions... purdue university engineering ...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3 Weitek P9000</td>\n      <td>weitek p9000 harris computer systems division ...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4 Shuttle Launch Question</td>\n      <td>shuttle launch question smithsonian astrophysi...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>11309</th>\n      <td>11309 Migraines and scans</td>\n      <td>migraines and scans distribution world inventi...</td>\n    </tr>\n    <tr>\n      <th>11310</th>\n      <td>11310 Screen Death Mac Plus 512</td>\n      <td>screen death mac plus 512 tufts university med...</td>\n    </tr>\n    <tr>\n      <th>11311</th>\n      <td>11311 Mounting CPU Cooler in vertical case</td>\n      <td>mounting cpu cooler in vertical case mail grou...</td>\n    </tr>\n    <tr>\n      <th>11312</th>\n      <td>11312 Sphere from 4 points</td>\n      <td>sphere from 4 points central research lab. hit...</td>\n    </tr>\n    <tr>\n      <th>11313</th>\n      <td>11313 stolen CBR900RR</td>\n      <td>stolen cbr900rr california institute of techno...</td>\n    </tr>\n  </tbody>\n</table>\n<p>11314 rows × 2 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 19
    }
   ],
   "source": [
    "df_news_save= df_news\n",
    "df_news_save = df_news_save.drop(['Word tokenize','Clean_Keyword'],axis=1)\n",
    "df_news_save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_news_save.to_csv(\"df_news_index.csv\", index=False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'thing,car,nntp,post,host,university,maryland,college,park,line,wonder,anyone,could,enlighten,car,saw,day,sport,car,look,late,early,call,bricklin,door,really,small,addition,front,bumper,separate,rest,body,know,anyone,tellme,model,name,engine,spec,year,production,car,make,history,whatever,info,funky,look,car,please,mail,il,bring,neighborhood,lerxst'"
      ]
     },
     "metadata": {},
     "execution_count": 21
    }
   ],
   "source": [
    "df_news.Clean_Keyword[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import operator\n",
    "\n",
    "## Create Vocabulary\n",
    "vocabulary = set()\n",
    "\n",
    "for doc in df_news.Clean_Keyword:\n",
    "    vocabulary.update(doc.split(','))\n",
    "\n",
    "vocabulary = list(vocabulary)\n",
    "\n",
    "# Intializating the tfIdf model\n",
    "tfidf = TfidfVectorizer(vocabulary=vocabulary,dtype=np.float32)\n",
    "\n",
    "# Fit the TfIdf model\n",
    "tfidf.fit(df_news.Clean_Keyword)\n",
    "\n",
    "# Transform the TfIdf model\n",
    "tfidf_tran=tfidf.transform(df_news.Clean_Keyword)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['tgi', 'riding', 'heatsinks', 'computerize']"
      ]
     },
     "metadata": {},
     "execution_count": 23
    }
   ],
   "source": [
    "vocabulary[0:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re, string\n",
    "import os \n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics.pairwise import linear_kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Version:  2.3.0\nEager mode:  True\nHub version:  0.7.0\nGPU is NOT AVAILABLE\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Version: \", tf.__version__)\n",
    "print(\"Eager mode: \", tf.executing_eagerly())\n",
    "print(\"Hub version: \", hub.__version__)\n",
    "print(\"GPU is\", \"available\" if tf.config.experimental.list_physical_devices(\"GPU\") else \"NOT AVAILABLE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Wall time: 5min 20s\n"
     ]
    }
   ],
   "source": [
    "%time model = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder/4\")\n",
    "def embed(input):\n",
    "    return model(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}